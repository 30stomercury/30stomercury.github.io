---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<!--add icon-->
<head>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
</head>

# About me
Hi! I'm Brian S. Yeh, 
currently a M.S. student in EE at National Tsing Hua University.
My current research is in the field of speech processing and machine learning with my advisor 
Prof. [Chi-Chun Lee](https://biic.ee.nthu.edu.tw/cclee.php). <br/>
Especially, I'm currently focus on the topic of end-to-end speech recognition and emotion recognition. 
Moreover, I'm also interested in NLP with research experiences in [text generation](https://arxiv.org/abs/2002.02095).

# Publication

**A Dialogical Emotion Decoder For Speech Emotion Recognition in Spoken Dialog** <br/>
   <u>Sung-Lin Yeh</u>, Yun-Shao Lin, Chi-Chun Lee<br/>
   *In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020, [Oral]*<br/>
   [<a style="text-decoration:none" href="https://github.com/30stomercury/Dialogical-Emotion-Decoding">
     <i class="fa fa-github" aria-hidden="true"></i> Github
   </a>|
   <a style="text-decoration:none" href="https://30stomercury.github.io/files/A_Dialogical_Emotion_Decoder_For_Speech_Emotion_Recognition_in_Spoken_Dialog.pdf">
     <i class="fa fa-file-pdf-o" aria-hidden="true"></i> Pdf
   </a>|
   <a style="text-decoration:none" href="https://www.youtube.com/watch?v=Ti4foNyrvzo&t=11s">
     <i class="fa fa-youtube-play" aria-hidden="true"></i> Video
   </a>]<br/>

**Attractive or Faithful? Popularity-Reinforced Learning for Inspired Headline Generation** <br/>
   Yun-Zhu Song, Hong-Han Shuai, <u>Sung-Lin Yeh</u>, Yi-Lun Wu, Lun-Wei Ku, Wen-Chih Peng<br/>
   *In the 34th AAAI Conference on Artificial Intelligence (AAAI) 2020, [Poster]*<br/>
   [<a style="text-decoration:none" href="https://arxiv.org/pdf/2002.02095.pdf">
     <i class="fa fa-file-pdf-o" aria-hidden="true"></i> Pdf
   </a>]<br/>

**Using Attention Networks and Adversarial Augmentation for Styrian Dialect Continuous Sleepiness and Baby Sound Recognition** <br/>
   <u>Sung-Lin Yeh</u> Gao-Yi Chao, Bo-Hao Su, Chi-Chun Lee and others<br/>
   *In Proceedings of the International Speech Communication Association (Interspeech) 2019, [Oral]*<br/>
   [<a style="text-decoration:none" href="https://github.com/30stomercury/IS19_ComParE_Sub-Challenge">
     <i class="fa fa-github" aria-hidden="true"></i> Github
   </a>|
   <a style="text-decoration:none" href="https://30stomercury.github.io/files/IS19_Challenge.pdf">
     <i class="fa fa-file-pdf-o" aria-hidden="true"></i> Pdf
   </a>]<br/>

**An Interaction-Aware Attention Network for Speech Emotion Recognition in Spoken Dialog** <br/>
   <u>Sung-Lin Yeh</u>, Yun-Shao Lin, Chi-Chun Lee<br/>
   *In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2019, [Poster]*<br/>
   [<a style="text-decoration:none" href="https://github.com/30stomercury/Interaction-aware_Attention_Network">
     <i class="fa fa-github" aria-hidden="true"></i> Github
   </a>|
   <a style="text-decoration:none" href="https://30stomercury.github.io/files/AN_INTERACTIO-AWARE_ATTENTION_NETWORK_FOR_SPEECH_EMOTION_RECOGNITION_IN_SPOKEN_DIALOGS.pdf">
     <i class="fa fa-file-pdf-o" aria-hidden="true"></i> Pdf
   </a>]<br/>

**Self-Assessed Affect Recognition using Fusion of Attentional BLSTM and Static Acoustic Features** <br/>
   Bo-Hao Su, <u>Sung-Lin Yeh</u>, Ming-Ya Ko, Huan-Yu Chen, Shun-Chang Zhong, Jeng-Lin Li, Chi-Chun Lee<br/>
   *In Proceedings of the International Speech Communication Association (Interspeech) 2018, [Oral]*<br/>
   [<a style="text-decoration:none" href="https://github.com/30stomercury/IS18_ComParE_Sub-Challenge">
     <i class="fa fa-github" aria-hidden="true"></i> Github
   </a>|
   <a style="text-decoration:none" href="https://30stomercury.github.io/files/IS18_Challenge.pdf">
     <i class="fa fa-file-pdf-o" aria-hidden="true"></i> Pdf
   </a>]<br/>


# Award

<p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;">
  <span style="flex: 0 0 auto">1st place of INTERSPEECH 2019 Computational Paralinguistics award.
   <a style="text-decoration:none" href="http://www.compare.openaudio.eu/winners/">
     [Link]
   </a>
  </span> <span style="flex:  0 0 auto">
  <i>Sep 2019</i>
  </span>
</p>
<p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;">
  <span style="flex: 0 0 auto">Novatek Scholarship (4000 USD).
  </span> <span style="flex:  0 0 auto">
  <i>July 2019</i>
  </span>
</p>
<p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;">
  <span style="flex: 0 0 auto">1st place of Mandarin Speech Recognition Challenge in Speech Signal Processing (EE664000).
  </span> <span style="flex:  0 0 auto">
  <i>Jan 2019</i>
  </span>
</p>
<p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;">
  <span style="flex: 0 0 auto">3rd place in Civil IoT Taiwan Hackson (16000 USD).
   <a style="text-decoration:none" href="http://isa.site.nthu.edu.tw/p/16-1182-149314.php?Lang=zh-tw">
     [Link]
   </a>
  </span> <span style="flex:  0 0 auto">
  <i>Oct 2018</i>
  </span>
</p>
<p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;">
  <span style="flex: 0 0 auto">NTHU EE Graduated Student Scholarship (2700 USD).
  </span> <span style="flex:  0 0 auto">
  <i>July 2018</i>
  </span>
</p>
<p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;">
  <span style="flex: 0 0 auto">College Student Research Scholarship, NSC
  </span> <span style="flex:  0 0 auto">
  <i>Jan 2018</i>
  </span>
</p>

# Teaching

<p style="display: flex; flex-direction: row; justify-content: space-between; margin: 0 0 0.5em;">
  <span style="flex: 0 0 auto">TA of Probability</span> <span style="flex:  0 0 auto">
  <i>NTHU EE, Spring 2020</i>  
  </span>
</p>
